{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GsDDSmTM6N9"
   },
   "source": [
    "# NER with BERT in Spark NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TZw9vE5M6OH"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRhmsIcCM6OH",
    "outputId": "27769f6f-954a-4407-9273-cfef5fe4a987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-30 15:56:55--  http://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2023-10-30 15:56:55--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1191 (1,2K) [text/plain]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>]   1,16K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-10-30 15:56:56 (56,8 MB/s) - written to stdout [1191/1191]\n",
      "\n",
      "Installing PySpark 3.2.3 and Spark NLP 5.1.4\n",
      "setup Colab for PySpark 3.2.3 and Spark NLP 5.1.4\n"
     ]
    }
   ],
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparknlp\n",
      "  Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n",
      "Requirement already satisfied: spark-nlp in /Users/ciprian.turcu/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages (from sparknlp) (5.1.4)\n",
      "Requirement already satisfied: numpy in /Users/ciprian.turcu/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages (from sparknlp) (1.25.2)\n",
      "Installing collected packages: sparknlp\n",
      "Successfully installed sparknlp-1.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sparknlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylCinY-tM6OI"
   },
   "source": [
    "## Import libraries and download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A6rnibyVM6OI"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9mD4BObM6OI",
    "outputId": "e901161a-b1a4-400b-d05d-fc5243cd5ae9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/ciprian.turcu/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if you have GPU\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# spark = sparknlp.start(gpu=True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43msparknlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark NLP version: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sparknlp\u001b[38;5;241m.\u001b[39mversion())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApache Spark version: \u001b[39m\u001b[38;5;124m\"\u001b[39m, spark\u001b[38;5;241m.\u001b[39mversion)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/__init__.py:300\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkRealTimeOutput()\u001b[38;5;241m.\u001b[39mspark_session\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m \u001b[43mstart_without_realtime_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/__init__.py:198\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m         builder\u001b[38;5;241m.\u001b[39mconfig(key, value)\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 339\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    111\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "# if you have GPU\n",
    "# spark = sparknlp.start(gpu=True)\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnfsIM1MM6OJ",
    "outputId": "bdab1de7-2d3b-48ce-91aa-2cdafc1c8d62"
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1458\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1108\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1108\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1379\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlretrieve\n\u001b[0;32m----> 3\u001b[0m \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meng.train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m urlretrieve(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng.testa\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train',\n",
    "           'eng.train')\n",
    "\n",
    "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa',\n",
    "           'eng.testa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ksp7t3WKM6OK",
    "outputId": "15e7883f-43b7-4629-b5f1-1d0c0a97c5c4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eng.train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meng.train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     c\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (c[:\u001b[38;5;241m200\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eng.train'"
     ]
    }
   ],
   "source": [
    "with open(\"eng.train\") as f:\n",
    "    c=f.read()\n",
    "\n",
    "print (c[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sNhRXZ1M6OK"
   },
   "source": [
    "## Building NER pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOgifXmgM6OL",
    "outputId": "335bd0a0-104e-4203-db9f-951cafff2fc3"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoNLL\n\u001b[0;32m----> 3\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mCoNLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadDataset(spark, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./eng.train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m training_data\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/training/conll.py:104\u001b[0m, in \u001b[0;36mCoNLL.__init__\u001b[0;34m(self, documentCol, sentenceCol, tokenCol, posCol, conllLabelIndex, conllPosIndex, conllDocIdCol, textCol, labelCol, explodeSentences, delimiter, includeDocId)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m              documentCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     92\u001b[0m              sentenceCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m              includeDocId\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    103\u001b[0m              ):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCoNLL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.training.CoNLL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdocumentCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msentenceCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtokenCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mposCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconllLabelIndex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconllPosIndex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconllDocIdCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtextCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mexplodeSentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mincludeDocId\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/extended_java_wrapper.py:27\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28msuper\u001b[39m(ExtendedJavaWrapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(java_obj)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/extended_java_wrapper.py:37\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_java_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_class, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py:62\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 62\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     64\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/util.py:38\u001b[0m, in \u001b[0;36m_jvm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvm\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load _jvm from SparkContext. Is SparkContext initialized?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "training_data = CoNLL().readDataset(spark, './eng.train')\n",
    "training_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6R1bF9QM6OL",
    "outputId": "7c0e2769-8ea5-45d4-f260-445358302195"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_data\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "training_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPHBKA1FM6OL"
   },
   "source": [
    "### Loading Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4IA9Rn0M6OL"
   },
   "source": [
    "In Spark NLP, we have four pre-trained variants of BERT: bert_base_uncased , bert_base_cased , bert_large_uncased , bert_large_cased, and many Smaller BERT models available on our [Models Hub](https://nlp.johnsnowlabs.com/models?q=bert&task=Embeddings). Which one to use depends on your use case, train set, and the complexity of the task you are trying to model.\n",
    "\n",
    "In the code snippet above, we basically load the bert_base_cased version from Spark NLP public resources and point the sentence and token columns in   setInputCols(). In short, BertEmbeddings() annotator will take sentence and token columns and populate Bert embeddings in bert column. In general, each word is translated to a 768-dimensional vector.\n",
    "\n",
    "As explained by the authors of official BERT paper, different BERT layers capture different information. The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore it may be biased to those targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-HV69OEM6OM",
    "outputId": "9e2d6acd-4e92-4a1c-eb21-6b6c1606844c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we use BERT Tiny\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bert_annotator \u001b[38;5;241m=\u001b[39m \u001b[43mBertEmbeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall_bert_L2_128\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      3\u001b[0m  \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m])\\\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241m.\u001b[39msetBatchSize(\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/annotator/embeddings/bert_embeddings.py:206\u001b[0m, in \u001b[0;36mBertEmbeddings.pretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Downloads and loads a pretrained model.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    The restored model\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResourceDownloader\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResourceDownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBertEmbeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_loc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/pretrained/resource_downloader.py:87\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Downloads and loads a model with the default downloader. Usually this method\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mdoes not need to be called directly, as it is called by the `pretrained()`\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mmethod of the annotator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Loaded pretrained annotator/pipeline\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m download started this may take some time.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GetResourceSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_loc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not find the model to download please check the name!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/__init__.py:390\u001b[0m, in \u001b[0;36m_GetResourceSize.__init__\u001b[0;34m(self, name, language, remote_loc)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, language, remote_loc):\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_GetResourceSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_loc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/extended_java_wrapper.py:27\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28msuper\u001b[39m(ExtendedJavaWrapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(java_obj)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/extended_java_wrapper.py:37\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_java_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_class, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py:62\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 62\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     64\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/util.py:38\u001b[0m, in \u001b[0;36m_jvm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvm\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load _jvm from SparkContext. Is SparkContext initialized?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "# we use BERT Tiny\n",
    "bert_annotator = BertEmbeddings.pretrained('small_bert_L2_128', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    ".setBatchSize(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVs0qpz7M6OM",
    "outputId": "eb5b7351-b8fb-4802-cb8b-4753029d127f"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoNLL\n\u001b[0;32m----> 3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mCoNLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadDataset(spark, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./eng.testa\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m test_data \u001b[38;5;241m=\u001b[39m bert_annotator\u001b[38;5;241m.\u001b[39mtransform(test_data)\n\u001b[1;32m      7\u001b[0m test_data\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/training/conll.py:104\u001b[0m, in \u001b[0;36mCoNLL.__init__\u001b[0;34m(self, documentCol, sentenceCol, tokenCol, posCol, conllLabelIndex, conllPosIndex, conllDocIdCol, textCol, labelCol, explodeSentences, delimiter, includeDocId)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m              documentCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     92\u001b[0m              sentenceCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m              includeDocId\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    103\u001b[0m              ):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCoNLL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.training.CoNLL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdocumentCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msentenceCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtokenCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mposCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconllLabelIndex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconllPosIndex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconllDocIdCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtextCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mexplodeSentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mincludeDocId\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/extended_java_wrapper.py:27\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28msuper\u001b[39m(ExtendedJavaWrapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(java_obj)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/internal/extended_java_wrapper.py:37\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_java_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_class, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjava_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py:62\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 62\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     64\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/util.py:38\u001b[0m, in \u001b[0;36m_jvm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvm\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load _jvm from SparkContext. Is SparkContext initialized?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "from sparknlp.training import CoNLL\n",
    "\n",
    "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
    "\n",
    "test_data = bert_annotator.transform(test_data)\n",
    "\n",
    "test_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mM-nQblLPkPf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's transform and save our test dataset for evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_withEmbeds.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# let's transform and save our test dataset for evaluation\n",
    "test_data.write.parquet(\"test_withEmbeds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vufW7YRM6ON",
    "outputId": "04c26d77-b833-44b7-9f78-c74de4db4136"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_data\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert.result\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert.embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel.result\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_data.select(\"bert.result\",\"bert.embeddings\",'label.result').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jJyP74zgM6OQ"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot load _jvm from SparkContext. Is SparkContext initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nerTagger \u001b[38;5;241m=\u001b[39m \u001b[43mNerDLApproach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;241m.\u001b[39msetLabelColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39msetMaxEpochs(\u001b[38;5;241m5\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39msetLr(\u001b[38;5;241m0.001\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39msetPo(\u001b[38;5;241m0.005\u001b[39m)\\\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;241m.\u001b[39msetBatchSize(\u001b[38;5;241m32\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;241m.\u001b[39msetEvaluationLogExtended(\u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;241m.\u001b[39msetEnableOutputLogs(\u001b[38;5;28;01mTrue\u001b[39;00m)\\\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;241m.\u001b[39msetTestDataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_withEmbeds.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m     14\u001b[0m     stages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     bert_annotator,\n\u001b[1;32m     16\u001b[0m     nerTagger\n\u001b[1;32m     17\u001b[0m   ])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/__init__.py:114\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/annotator/ner/ner_dl.py:380\u001b[0m, in \u001b[0;36mNerDLApproach.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNerDLApproach\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     uc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m    383\u001b[0m         minEpochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    384\u001b[0m         maxEpochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         bestModelMetric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/__init__.py:114\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/sparknlp/common/annotator_approach.py:31\u001b[0m, in \u001b[0;36mAnnotatorApproach.__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     29\u001b[0m _internal\u001b[38;5;241m.\u001b[39mParamsGettersSetters\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(lazyAnnotator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/wrapper.py:62\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 62\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     64\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-engineer-assignment1-qOUDlTmM-py3.11/lib/python3.11/site-packages/pyspark/ml/util.py:38\u001b[0m, in \u001b[0;36m_jvm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvm\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load _jvm from SparkContext. Is SparkContext initialized?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
     ]
    }
   ],
   "source": [
    "nerTagger = NerDLApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setLr(0.001)\\\n",
    "  .setPo(0.005)\\\n",
    "  .setBatchSize(32)\\\n",
    "  .setEvaluationLogExtended(True) \\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setTestDataset(\"test_withEmbeds.parquet\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    bert_annotator,\n",
    "    nerTagger\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYJT6mHLM6OQ"
   },
   "source": [
    "You can also set learning rate ( setLr ), learning rate decay coefficient ( setPo ), setBatchSize and setDropout rate. Please see the [official APIs](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/ner/dl/NerDLApproach.html) for the entire list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wCMb1XmM6OR",
    "outputId": "628d5f62-868a-4faa-8b5e-a3792a9b5e4f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ner_model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtLbO6RfP-y7",
    "outputId": "75bbd436-7b82-4205-f86e-e07f9194af53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /root/annotator_logs/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l /root/annotator_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zAvs21PQDU1",
    "outputId": "e1fa4f56-ac31-4299-e29e-056cda24a139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /root/annotator_logs/NerDLApproach_16a0e7b3577f.log: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat /root/annotator_logs/NerDLApproach_16a0e7b3577f.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo-GNidLr1wt"
   },
   "source": [
    "**Some notes:**\n",
    "- we used the smallest BERT model called BERT Tiny\n",
    "- it's very small and requires less memory among Transformers\n",
    "- if you have more memory or access to accelerated hardware please choose a larger BERT model for higher accuracy\n",
    "- you can also set higher Epoch to reach our STOA metrics\n",
    "\n",
    "We chose the smallest BERT model with only 5 Epochs for the sake of this tutorial within this small Colab VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApUISh0AM6OR"
   },
   "outputs": [],
   "source": [
    "# let's save our trained NER model on disk\n",
    "# so we can load it in a new session or move it to another location\n",
    "# since we fit NerDL model inside the pipeline, we can access it via stages\n",
    "ner_model.stages[1].write().overwrite().save('./NER_bert_20200219')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WehIZ-d9s5Ok",
    "outputId": "26116dd1-b22e-4c7f-85df-cfc68a00eb1b"
   },
   "outputs": [],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiQvE5pmM6OS",
    "outputId": "6264b27c-2399-49a3-f6ac-17c3698e8177"
   },
   "outputs": [],
   "source": [
    "# let's only feed sentence and token from our test dataset\n",
    "predictions = ner_model.transform(test_data.select(\"sentence\", \"token\", \"label\"))\n",
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eTDVoCxM6OS",
    "outputId": "80147911-7888-408e-f963-ac8510db18c6"
   },
   "outputs": [],
   "source": [
    "predictions.select('token.result','label.result','ner.result').show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzC5CDUwM6OS",
    "outputId": "da65b6e2-6725-4e03-8fff-3c377b0a6e35"
   },
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzYP3TGfM6OT",
    "outputId": "36490d72-4c60-4649-ce01-f10f3222f3cd"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "        F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjaahMbzuV4A"
   },
   "source": [
    "# Convert to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "z-M0PZd6M6OU",
    "outputId": "84421d33-68ed-4259-a3f5-7c0a77ca831a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = predictions.select('token.result','label.result','ner.result').toPandas()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N47oS8WM6OY"
   },
   "source": [
    "## Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZMRW44DM6OY"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "bert = BertEmbeddings.pretrained('small_bert_L2_128', 'en') \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    " .setCaseSensitive(True)\n",
    "\n",
    "loaded_ner_model = NerDLModel.load(\"NER_bert_20200219\")\\\n",
    " .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    " .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        bert,\n",
    "        loaded_ner_model,\n",
    "        converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmtzkvh0M6OZ"
   },
   "outputs": [],
   "source": [
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "empty_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-ntORrdM6OZ"
   },
   "outputs": [],
   "source": [
    "prediction_model = ner_prediction_pipeline.fit(empty_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6hxHQBLM6OZ"
   },
   "outputs": [],
   "source": [
    "text = \"Peter Parker is a nice guy and lives in New York.\"\n",
    "sample_data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "sample_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os-XoJCtM6Oa"
   },
   "outputs": [],
   "source": [
    "\n",
    "preds = prediction_model.transform(sample_data)\n",
    "\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flcEnI6BM6Oa"
   },
   "outputs": [],
   "source": [
    "preds.select('ner_span.result').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95ryt1wEM6Oa"
   },
   "outputs": [],
   "source": [
    "\n",
    "preds.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\")) \\\n",
    ".select(F.expr(\"entities['0']\").alias(\"chunk\"),\n",
    "        F.expr(\"entities['1'].entity\").alias(\"entity\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT2ojwJHM6Oa"
   },
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "loaded_ner_model = NerDLModel.load(\"NER_bert_20200219\")\\\n",
    " .setInputCols([\"sentence\", \"token\", \"glove\"])\\\n",
    " .setOutputCol(\"ner\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
    "  .setOutputCol(\"ner_span\")\n",
    "\n",
    "glove_ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        glove,\n",
    "        loaded_ner_model,\n",
    "        converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iw71QpnXM6Ob"
   },
   "outputs": [],
   "source": [
    "glove_prediction_model = glove_ner_prediction_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqCcz7rwM6Ob"
   },
   "outputs": [],
   "source": [
    "\n",
    "preds = glove_prediction_model.transform(sample_data)\n",
    "\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsoWGO7MM6Ob"
   },
   "outputs": [],
   "source": [
    "\n",
    "preds.select(F.explode(F.arrays_zip(\"ner_span.result\",\"ner_span.metadata\")).alias(\"entities\")) \\\n",
    ".select(F.expr(\"entities['0']\").alias(\"chunk\"),\n",
    "        F.expr(\"entities['1'].entity\").alias(\"entity\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWIys6BM6Ob"
   },
   "source": [
    "### Pretrained Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOP1riCCM6Oc"
   },
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pretrained_pipeline = PretrainedPipeline('recognize_entities_dl', lang='en')\n",
    "\n",
    "#onto_recognize_entities_sm\n",
    "#explain_document_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdh4U83kM6Oc"
   },
   "outputs": [],
   "source": [
    "text = \"The Mona Lisa is a 16th century oil painting created by Leonardo. It's held at the Louvre in Paris.\"\n",
    "\n",
    "result = pretrained_pipeline.annotate(text)\n",
    "\n",
    "list(zip(result['token'], result['ner']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-F-QJzCM6Oc"
   },
   "outputs": [],
   "source": [
    "pretrained_pipeline2 = PretrainedPipeline('explain_document_dl', lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da9Ql6-hM6Oc"
   },
   "outputs": [],
   "source": [
    "text = \"The Mona Lisa is a 16th centry oil painting created by Leonrdo. It's held at the Louvre in Paris.\"\n",
    "\n",
    "result2 = pretrained_pipeline2.annotate(text)\n",
    "\n",
    "result2\n",
    "list(zip(result2['token'],  result2['checked'], result2['pos'], result2['ner'],  result2['lemma'],  result2['stem']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UotW3Nr_M6Od"
   },
   "outputs": [],
   "source": [
    "xx= pretrained_pipeline2.fullAnnotate(text)\n",
    "\n",
    "[(n.result, n.metadata['entity']) for n in xx['ner_span']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbY3ZstAM6OV"
   },
   "source": [
    "### with Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40YZFzD6M6OW"
   },
   "outputs": [],
   "source": [
    "glove = WordEmbeddingsModel().pretrained() \\\n",
    " .setInputCols([\"sentence\",'token'])\\\n",
    " .setOutputCol(\"glove\")\\\n",
    " .setCaseSensitive(False)\n",
    "\n",
    "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
    "\n",
    "test_data = glove.transform(test_data.limit(1000))\n",
    "\n",
    "test_data.write.parquet(\"test_withGloveEmbeds.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygjYFiZMM6OW"
   },
   "outputs": [],
   "source": [
    "nerTagger.setInputCols([\"sentence\", \"token\", \"glove\"])\n",
    "nerTagger.setTestDataset(\"test_withGloveEmbeds.parquet\")\n",
    "\n",
    "glove_pipeline = Pipeline(\n",
    "    stages = [\n",
    "    glove,\n",
    "    nerTagger\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJLeqlq3M6OW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ner_model_v3 = glove_pipeline.fit(training_data.limit(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix0O0YPwM6OX"
   },
   "outputs": [],
   "source": [
    "predictions_v3 = ner_model_v3.transform(test_data.limit(10))\n",
    "\n",
    "# test_data.sample(False,0.1,0)\n",
    "\n",
    "predictions_v3.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
    ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
    "        F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26z4CW7QM6OX"
   },
   "outputs": [],
   "source": [
    "np.array (predictions.select('token.result').take(1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGZjA_pBM6OX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokens = np.array (predictions.select('token.result').take(1))[0][0]\n",
    "ground = np.array (predictions.select('label.result').take(1))[0][0]\n",
    "label_bert_0 = np.array (predictions.select('ner.result').take(1))[0][0]\n",
    "label_bert_2 = np.array (predictions_v2.select('ner.result').take(1))[0][0]\n",
    "label_glove = np.array (predictions_v3.select('ner.result').take(1))[0][0]\n",
    "\n",
    "pd.DataFrame({'token':tokens,\n",
    "              'ground':ground,\n",
    "              'label_bert_0':label_bert_0,\n",
    "              'label_bert_2':label_bert_2,\n",
    "              'label_glove':label_glove})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwAHj6mPM6Od"
   },
   "source": [
    "## Using your own custom Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC8RR7iXM6Od"
   },
   "outputs": [],
   "source": [
    "custom_embeddings = WordEmbeddings()\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"glove\")\\\n",
    "  .setStoragePath('/Users/vkocaman/cache_pretrained/PubMed-shuffle-win-2.bin', \"BINARY\")\\\n",
    ".setDimension(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVkJgZnRM6Od",
    "outputId": "51edbce3-679e-4626-95ab-c3070f17e533"
   },
   "outputs": [],
   "source": [
    "custom_embeddings.fit(training_data.limit(10)).transform(training_data.limit(10)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKp_pARIM6Oe"
   },
   "source": [
    "## creating your own CoNLL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdCxFU2HM6Oe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "def get_ann_pipeline ():\n",
    "    \n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "    sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentence')\\\n",
    "        .setCustomBounds(['\\n'])\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"sentence\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "    pos = PerceptronModel.pretrained() \\\n",
    "              .setInputCols([\"sentence\", \"token\"]) \\\n",
    "              .setOutputCol(\"pos\")\n",
    "    \n",
    "    embeddings = WordEmbeddingsModel.pretrained()\\\n",
    "          .setInputCols([\"sentence\", \"token\"])\\\n",
    "          .setOutputCol(\"embeddings\")\n",
    "\n",
    "    ner_model = NerDLModel.pretrained() \\\n",
    "          .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "          .setOutputCol(\"ner\")\n",
    "\n",
    "    ner_converter = NerConverter()\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "      .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "    ner_pipeline = Pipeline(\n",
    "        stages = [\n",
    "            document_assembler,\n",
    "            sentence,\n",
    "            tokenizer,\n",
    "            pos,\n",
    "            embeddings,\n",
    "            ner_model,\n",
    "            ner_converter\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "    ner_pipelineFit = ner_pipeline.fit(empty_data)\n",
    "\n",
    "    ner_lp_pipeline = LightPipeline(ner_pipelineFit)\n",
    "\n",
    "    print (\"Spark NLP NER lightpipeline is created\")\n",
    "\n",
    "    return ner_lp_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMgY0pEhM6Oe",
    "outputId": "680f7dfb-e9ae-43d2-c478-6761d61d7cf6"
   },
   "outputs": [],
   "source": [
    "conll_pipeline = get_ann_pipeline ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aYv-WM1M6Oe",
    "outputId": "f3fdf133-4750-4a67-b2f4-5540379c4b72"
   },
   "outputs": [],
   "source": [
    "parsed = conll_pipeline.annotate (\"Peter Parker is a nice guy and lives in New York.\")\n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNpOPXUSM6Of",
    "outputId": "b24a0cee-31a9-42d3-c8e3-19876d929f9c"
   },
   "outputs": [],
   "source": [
    "conll_lines=''\n",
    "\n",
    "for token, pos, ner in zip(parsed['token'],parsed['pos'],parsed['ner']):\n",
    "\n",
    "    conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, ner)\n",
    "\n",
    "\n",
    "print(conll_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfKkZheoM6Of"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5N47oS8WM6OY",
    "zYWIys6BM6Ob",
    "WbY3ZstAM6OV",
    "WwAHj6mPM6Od",
    "YKp_pARIM6Oe"
   ],
   "name": "3.NER_with_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
